{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bootcamp - Mod4 - Aula04.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOeF2jLW9wvVXR7yjnmkPE1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gcochlar/Bootcamp_DataScience/blob/main/Modulo_04/Bootcamp_Mod4_Aula04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZKKSXVwpVfu"
      },
      "source": [
        "<img src='https://drive.google.com/uc?id=16O9eMrtSeRDnDpZgmneXj34eIClCntvg'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKeX4KHBZUkZ"
      },
      "source": [
        "# **Tratamento, Análise e** ***Machine Learning*** **Aplicado**\n",
        "\n",
        "---\n",
        "\n",
        "Esse notebook servirá para o acompanhamento das aulas do Módulo 04 do ***Bootcamp de Data Science Aplicada*** realizado pela Alura.\n",
        "\n",
        "Este módulo dará início ao desenvolvimento de um projeto prático que passará por todo o *workflow* em *Data Science*, do entendimento do problema, tratamento e análise dos dados até a proposta de solução utilizando ***Machine Learning***.\n",
        "\n",
        "Vamos trabalhar com dados da **COVID-19** do **Hospital Sírio Libanês**, focando este módulo no tratamento e análise de dados para entender profundamente o problema que estamos lidando e propor possíveis soluções.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOJYQLwvbh7E"
      },
      "source": [
        "## **Aula 01 -** ***Machine Learning*** **e Saúde**\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnkFSRniaFYT"
      },
      "source": [
        "import pandas as pd\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Ei98MiwRvWH"
      },
      "source": [
        "url = 'https://github.com/gcochlar/Bootcamp_DataScience/blob/main/dados/Kaggle_Sirio_Libanes_ICU_Prediction.xlsx?raw=true'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgHJeIs0aIcb"
      },
      "source": [
        "dados = pd.read_excel(url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "q04C65RUaQzK",
        "outputId": "b71708a3-464c-4dd6-96f1-057d6fcf18c1"
      },
      "source": [
        "dados.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PATIENT_VISIT_IDENTIFIER</th>\n",
              "      <th>AGE_ABOVE65</th>\n",
              "      <th>AGE_PERCENTIL</th>\n",
              "      <th>GENDER</th>\n",
              "      <th>DISEASE GROUPING 1</th>\n",
              "      <th>DISEASE GROUPING 2</th>\n",
              "      <th>DISEASE GROUPING 3</th>\n",
              "      <th>DISEASE GROUPING 4</th>\n",
              "      <th>DISEASE GROUPING 5</th>\n",
              "      <th>DISEASE GROUPING 6</th>\n",
              "      <th>HTN</th>\n",
              "      <th>IMMUNOCOMPROMISED</th>\n",
              "      <th>OTHER</th>\n",
              "      <th>ALBUMIN_MEDIAN</th>\n",
              "      <th>ALBUMIN_MEAN</th>\n",
              "      <th>ALBUMIN_MIN</th>\n",
              "      <th>ALBUMIN_MAX</th>\n",
              "      <th>ALBUMIN_DIFF</th>\n",
              "      <th>BE_ARTERIAL_MEDIAN</th>\n",
              "      <th>BE_ARTERIAL_MEAN</th>\n",
              "      <th>BE_ARTERIAL_MIN</th>\n",
              "      <th>BE_ARTERIAL_MAX</th>\n",
              "      <th>BE_ARTERIAL_DIFF</th>\n",
              "      <th>BE_VENOUS_MEDIAN</th>\n",
              "      <th>BE_VENOUS_MEAN</th>\n",
              "      <th>BE_VENOUS_MIN</th>\n",
              "      <th>BE_VENOUS_MAX</th>\n",
              "      <th>BE_VENOUS_DIFF</th>\n",
              "      <th>BIC_ARTERIAL_MEDIAN</th>\n",
              "      <th>BIC_ARTERIAL_MEAN</th>\n",
              "      <th>BIC_ARTERIAL_MIN</th>\n",
              "      <th>BIC_ARTERIAL_MAX</th>\n",
              "      <th>BIC_ARTERIAL_DIFF</th>\n",
              "      <th>BIC_VENOUS_MEDIAN</th>\n",
              "      <th>BIC_VENOUS_MEAN</th>\n",
              "      <th>BIC_VENOUS_MIN</th>\n",
              "      <th>BIC_VENOUS_MAX</th>\n",
              "      <th>BIC_VENOUS_DIFF</th>\n",
              "      <th>BILLIRUBIN_MEDIAN</th>\n",
              "      <th>BILLIRUBIN_MEAN</th>\n",
              "      <th>...</th>\n",
              "      <th>DIMER_MAX</th>\n",
              "      <th>DIMER_DIFF</th>\n",
              "      <th>BLOODPRESSURE_DIASTOLIC_MEAN</th>\n",
              "      <th>BLOODPRESSURE_SISTOLIC_MEAN</th>\n",
              "      <th>HEART_RATE_MEAN</th>\n",
              "      <th>RESPIRATORY_RATE_MEAN</th>\n",
              "      <th>TEMPERATURE_MEAN</th>\n",
              "      <th>OXYGEN_SATURATION_MEAN</th>\n",
              "      <th>BLOODPRESSURE_DIASTOLIC_MEDIAN</th>\n",
              "      <th>BLOODPRESSURE_SISTOLIC_MEDIAN</th>\n",
              "      <th>HEART_RATE_MEDIAN</th>\n",
              "      <th>RESPIRATORY_RATE_MEDIAN</th>\n",
              "      <th>TEMPERATURE_MEDIAN</th>\n",
              "      <th>OXYGEN_SATURATION_MEDIAN</th>\n",
              "      <th>BLOODPRESSURE_DIASTOLIC_MIN</th>\n",
              "      <th>BLOODPRESSURE_SISTOLIC_MIN</th>\n",
              "      <th>HEART_RATE_MIN</th>\n",
              "      <th>RESPIRATORY_RATE_MIN</th>\n",
              "      <th>TEMPERATURE_MIN</th>\n",
              "      <th>OXYGEN_SATURATION_MIN</th>\n",
              "      <th>BLOODPRESSURE_DIASTOLIC_MAX</th>\n",
              "      <th>BLOODPRESSURE_SISTOLIC_MAX</th>\n",
              "      <th>HEART_RATE_MAX</th>\n",
              "      <th>RESPIRATORY_RATE_MAX</th>\n",
              "      <th>TEMPERATURE_MAX</th>\n",
              "      <th>OXYGEN_SATURATION_MAX</th>\n",
              "      <th>BLOODPRESSURE_DIASTOLIC_DIFF</th>\n",
              "      <th>BLOODPRESSURE_SISTOLIC_DIFF</th>\n",
              "      <th>HEART_RATE_DIFF</th>\n",
              "      <th>RESPIRATORY_RATE_DIFF</th>\n",
              "      <th>TEMPERATURE_DIFF</th>\n",
              "      <th>OXYGEN_SATURATION_DIFF</th>\n",
              "      <th>BLOODPRESSURE_DIASTOLIC_DIFF_REL</th>\n",
              "      <th>BLOODPRESSURE_SISTOLIC_DIFF_REL</th>\n",
              "      <th>HEART_RATE_DIFF_REL</th>\n",
              "      <th>RESPIRATORY_RATE_DIFF_REL</th>\n",
              "      <th>TEMPERATURE_DIFF_REL</th>\n",
              "      <th>OXYGEN_SATURATION_DIFF_REL</th>\n",
              "      <th>WINDOW</th>\n",
              "      <th>ICU</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>60th</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.086420</td>\n",
              "      <td>-0.230769</td>\n",
              "      <td>-0.283019</td>\n",
              "      <td>-0.593220</td>\n",
              "      <td>-0.285714</td>\n",
              "      <td>0.736842</td>\n",
              "      <td>0.086420</td>\n",
              "      <td>-0.230769</td>\n",
              "      <td>-0.283019</td>\n",
              "      <td>-0.586207</td>\n",
              "      <td>-0.285714</td>\n",
              "      <td>0.736842</td>\n",
              "      <td>0.237113</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>-0.162393</td>\n",
              "      <td>-0.500000</td>\n",
              "      <td>0.208791</td>\n",
              "      <td>0.898990</td>\n",
              "      <td>-0.247863</td>\n",
              "      <td>-0.459459</td>\n",
              "      <td>-0.432836</td>\n",
              "      <td>-0.636364</td>\n",
              "      <td>-0.420290</td>\n",
              "      <td>0.736842</td>\n",
              "      <td>-1.00000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0-2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>60th</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>-0.230769</td>\n",
              "      <td>-0.132075</td>\n",
              "      <td>-0.593220</td>\n",
              "      <td>0.535714</td>\n",
              "      <td>0.578947</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>-0.230769</td>\n",
              "      <td>-0.132075</td>\n",
              "      <td>-0.586207</td>\n",
              "      <td>0.535714</td>\n",
              "      <td>0.578947</td>\n",
              "      <td>0.443299</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>-0.025641</td>\n",
              "      <td>-0.500000</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.838384</td>\n",
              "      <td>-0.076923</td>\n",
              "      <td>-0.459459</td>\n",
              "      <td>-0.313433</td>\n",
              "      <td>-0.636364</td>\n",
              "      <td>0.246377</td>\n",
              "      <td>0.578947</td>\n",
              "      <td>-1.00000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>2-4</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>60th</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.605263</td>\n",
              "      <td>0.605263</td>\n",
              "      <td>0.605263</td>\n",
              "      <td>0.605263</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-0.317073</td>\n",
              "      <td>-0.317073</td>\n",
              "      <td>-0.317073</td>\n",
              "      <td>-0.317073</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-0.317073</td>\n",
              "      <td>-0.317073</td>\n",
              "      <td>-0.317073</td>\n",
              "      <td>-0.317073</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-0.938950</td>\n",
              "      <td>-0.938950</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.994912</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4-6</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>60th</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.107143</td>\n",
              "      <td>0.736842</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.107143</td>\n",
              "      <td>0.736842</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.318681</td>\n",
              "      <td>0.898990</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.275362</td>\n",
              "      <td>0.736842</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>6-12</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>60th</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-0.871658</td>\n",
              "      <td>-0.871658</td>\n",
              "      <td>-0.871658</td>\n",
              "      <td>-0.871658</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-0.863874</td>\n",
              "      <td>-0.863874</td>\n",
              "      <td>-0.863874</td>\n",
              "      <td>-0.863874</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-0.317073</td>\n",
              "      <td>-0.317073</td>\n",
              "      <td>-0.317073</td>\n",
              "      <td>-0.317073</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-0.414634</td>\n",
              "      <td>-0.414634</td>\n",
              "      <td>-0.414634</td>\n",
              "      <td>-0.414634</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-0.979069</td>\n",
              "      <td>-0.979069</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.996762</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-0.243021</td>\n",
              "      <td>-0.338537</td>\n",
              "      <td>-0.213031</td>\n",
              "      <td>-0.317859</td>\n",
              "      <td>0.033779</td>\n",
              "      <td>0.665932</td>\n",
              "      <td>-0.283951</td>\n",
              "      <td>-0.376923</td>\n",
              "      <td>-0.188679</td>\n",
              "      <td>-0.379310</td>\n",
              "      <td>0.035714</td>\n",
              "      <td>0.631579</td>\n",
              "      <td>-0.340206</td>\n",
              "      <td>-0.4875</td>\n",
              "      <td>-0.572650</td>\n",
              "      <td>-0.857143</td>\n",
              "      <td>0.098901</td>\n",
              "      <td>0.797980</td>\n",
              "      <td>-0.076923</td>\n",
              "      <td>0.286486</td>\n",
              "      <td>0.298507</td>\n",
              "      <td>0.272727</td>\n",
              "      <td>0.362319</td>\n",
              "      <td>0.947368</td>\n",
              "      <td>-0.33913</td>\n",
              "      <td>0.325153</td>\n",
              "      <td>0.114504</td>\n",
              "      <td>0.176471</td>\n",
              "      <td>-0.238095</td>\n",
              "      <td>-0.818182</td>\n",
              "      <td>-0.389967</td>\n",
              "      <td>0.407558</td>\n",
              "      <td>-0.230462</td>\n",
              "      <td>0.096774</td>\n",
              "      <td>-0.242282</td>\n",
              "      <td>-0.814433</td>\n",
              "      <td>ABOVE_12</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 231 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   PATIENT_VISIT_IDENTIFIER  AGE_ABOVE65  ...    WINDOW  ICU\n",
              "0                         0            1  ...       0-2    0\n",
              "1                         0            1  ...       2-4    0\n",
              "2                         0            1  ...       4-6    0\n",
              "3                         0            1  ...      6-12    0\n",
              "4                         0            1  ...  ABOVE_12    1\n",
              "\n",
              "[5 rows x 231 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehmqxUrBe2r5"
      },
      "source": [
        "A primeira aula serviu para explicar o funcionamento do **Kaggle** e nos ajudar a localizar o desafio do **Hospital Sírio Libanês** que está postado lá ([link aqui](https://www.kaggle.com/S%C3%ADrio-Libanes/covid19)).\n",
        "\n",
        "Também nos mostrou como interpretar inicialmente os dados do desafio e como criar e trabalhar com o notebook diretamente dentro do **Kaggle**.\n",
        "\n",
        "Além disso, foi explicada a diferença entre ***Algoritmo de Classificação Binária***, que será utilizado nesse desafio e ***Algoritmo de Regressão***.\n",
        "\n",
        "Na ***Classificação Binária*** buscamos uma resposta *booleana*, o resultado acontece ou não (1 e 0). No presente caso, o paciente vai precisar de internação em UTI ou não.\n",
        "\n",
        "Na ***Regressão*** buscamos um resultado numa faixa contínua de resultados. Por exemplo, um preço de venda para um apartamento.\n",
        "\n",
        "Em função da continuidade e para continuar postando os notebooks no **GitHub**, vou continuar usando o **Colab**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUIEN22E4H0y"
      },
      "source": [
        "#### **Desafio - Explorar o Kaggle, os datasets, competições e discussões para verificar o quão rico é esta plataforma**\n",
        "---\n",
        "\n",
        "Já conhecia o **Kaggle** desde a *Imersão em Data Science* da **Alura**, mas aproveitei a oportunidade para navegar mais um pouco pelo site.\n",
        "\n",
        "Achei interessante a disponibilidade de bases de dados de assuntos diversos como cinema, quadrinhos e música, além de assuntos mais sérios como já era esperado.\n",
        "\n",
        "Não tinha notado também que o **Kaggle** oferece alguns cursos, pretendo retornar posteriormente para ver o que eles disponibilizam em termos de ***Machine Learning***."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mo74ePkhqBV"
      },
      "source": [
        "#### **Desafio - Leia a descrição do problema proposta pelo Sírio Libanês, tentando entender qual é o real problema. Antes de continuar para as próximas aulas, debata no Discord suas conclusões sobre o projeto**\n",
        "---\n",
        "\n",
        "O problema proposto no desafio estabelece duas tarefas diferentes. Vi que no **Discord** houve uma certa discussão a respeito de serem a mesma coisa ou não, mas o meu entendimento é de que são tarefas diferentes mesmo.\n",
        "\n",
        ">**Task 01**\n",
        ">\n",
        ">Predict admission to the ICU of confirmed COVID-19 cases.\n",
        ">\n",
        ">Based on the data available, is it feasible to predict which patients will need intensive care unit support?\n",
        ">\n",
        ">The aim is to provide tertiary and quarternary hospitals with the most accurate answer, so ICU resources can be arranged or patient transfer can be scheduled.\n",
        "\n",
        "Essa tarefa busca maximizar o acerto de casos positivos minimizando os falsos negativos, ou seja, acertar o máximo de casos de pacientes que irão precisar de UTI, para evitar o sub- ou superdimensionamento da estrutura.\n",
        "\n",
        ">**Task 02**\n",
        ">\n",
        ">Predict NOT admission to the ICU of confirmed COVID-19 cases.\n",
        ">\n",
        ">Based on the subsample of widely available data, is it feasible to predict which patients will need intensive care unit support?\n",
        ">\n",
        ">The aim is to provide local and temporary hospitals a good enough answer, so frontline physicians can safely discharge and remotely follow up with these patients.\n",
        "\n",
        "Essa tarefa busca maximizar o acerto de casos negativos minimizando os falsos positivos, ou seja, acertar o máximo de casos que não irão precisar de UTI e que poderiam receber alta e ser monitorados à distância, liberando a estrutura hospitalar para receber mais pacientes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03D-rOIN5dKM"
      },
      "source": [
        "### **O que aprendemos nessa aula?**\n",
        "---\n",
        "\n",
        ">* O que é o **Kaggle** e como utilizar;\n",
        "* Como interpretar o problema proposto pelo **Sírio Libanês**;\n",
        "* O que é uma Classificação Binária.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tb_EiSf1rZ9-"
      },
      "source": [
        "## **Aula 02 - Desenvolvimento no** ***Kaggle***\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjG0KzP-n9MF"
      },
      "source": [
        "Apesar da aula continuar usando o **Kaggle**, vou permanecer no **Colab**.\n",
        "\n",
        "Vamos começar analisando o que temos na base de dados importada."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "Sm0awjyun8zf",
        "outputId": "5fc7c737-6d9d-4e7c-ee72-bb39a9181eb0"
      },
      "source": [
        "dados.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PATIENT_VISIT_IDENTIFIER</th>\n",
              "      <th>AGE_ABOVE65</th>\n",
              "      <th>GENDER</th>\n",
              "      <th>DISEASE GROUPING 1</th>\n",
              "      <th>DISEASE GROUPING 2</th>\n",
              "      <th>DISEASE GROUPING 3</th>\n",
              "      <th>DISEASE GROUPING 4</th>\n",
              "      <th>DISEASE GROUPING 5</th>\n",
              "      <th>DISEASE GROUPING 6</th>\n",
              "      <th>HTN</th>\n",
              "      <th>IMMUNOCOMPROMISED</th>\n",
              "      <th>OTHER</th>\n",
              "      <th>ALBUMIN_MEDIAN</th>\n",
              "      <th>ALBUMIN_MEAN</th>\n",
              "      <th>ALBUMIN_MIN</th>\n",
              "      <th>ALBUMIN_MAX</th>\n",
              "      <th>ALBUMIN_DIFF</th>\n",
              "      <th>BE_ARTERIAL_MEDIAN</th>\n",
              "      <th>BE_ARTERIAL_MEAN</th>\n",
              "      <th>BE_ARTERIAL_MIN</th>\n",
              "      <th>BE_ARTERIAL_MAX</th>\n",
              "      <th>BE_ARTERIAL_DIFF</th>\n",
              "      <th>BE_VENOUS_MEDIAN</th>\n",
              "      <th>BE_VENOUS_MEAN</th>\n",
              "      <th>BE_VENOUS_MIN</th>\n",
              "      <th>BE_VENOUS_MAX</th>\n",
              "      <th>BE_VENOUS_DIFF</th>\n",
              "      <th>BIC_ARTERIAL_MEDIAN</th>\n",
              "      <th>BIC_ARTERIAL_MEAN</th>\n",
              "      <th>BIC_ARTERIAL_MIN</th>\n",
              "      <th>BIC_ARTERIAL_MAX</th>\n",
              "      <th>BIC_ARTERIAL_DIFF</th>\n",
              "      <th>BIC_VENOUS_MEDIAN</th>\n",
              "      <th>BIC_VENOUS_MEAN</th>\n",
              "      <th>BIC_VENOUS_MIN</th>\n",
              "      <th>BIC_VENOUS_MAX</th>\n",
              "      <th>BIC_VENOUS_DIFF</th>\n",
              "      <th>BILLIRUBIN_MEDIAN</th>\n",
              "      <th>BILLIRUBIN_MEAN</th>\n",
              "      <th>BILLIRUBIN_MIN</th>\n",
              "      <th>...</th>\n",
              "      <th>DIMER_MIN</th>\n",
              "      <th>DIMER_MAX</th>\n",
              "      <th>DIMER_DIFF</th>\n",
              "      <th>BLOODPRESSURE_DIASTOLIC_MEAN</th>\n",
              "      <th>BLOODPRESSURE_SISTOLIC_MEAN</th>\n",
              "      <th>HEART_RATE_MEAN</th>\n",
              "      <th>RESPIRATORY_RATE_MEAN</th>\n",
              "      <th>TEMPERATURE_MEAN</th>\n",
              "      <th>OXYGEN_SATURATION_MEAN</th>\n",
              "      <th>BLOODPRESSURE_DIASTOLIC_MEDIAN</th>\n",
              "      <th>BLOODPRESSURE_SISTOLIC_MEDIAN</th>\n",
              "      <th>HEART_RATE_MEDIAN</th>\n",
              "      <th>RESPIRATORY_RATE_MEDIAN</th>\n",
              "      <th>TEMPERATURE_MEDIAN</th>\n",
              "      <th>OXYGEN_SATURATION_MEDIAN</th>\n",
              "      <th>BLOODPRESSURE_DIASTOLIC_MIN</th>\n",
              "      <th>BLOODPRESSURE_SISTOLIC_MIN</th>\n",
              "      <th>HEART_RATE_MIN</th>\n",
              "      <th>RESPIRATORY_RATE_MIN</th>\n",
              "      <th>TEMPERATURE_MIN</th>\n",
              "      <th>OXYGEN_SATURATION_MIN</th>\n",
              "      <th>BLOODPRESSURE_DIASTOLIC_MAX</th>\n",
              "      <th>BLOODPRESSURE_SISTOLIC_MAX</th>\n",
              "      <th>HEART_RATE_MAX</th>\n",
              "      <th>RESPIRATORY_RATE_MAX</th>\n",
              "      <th>TEMPERATURE_MAX</th>\n",
              "      <th>OXYGEN_SATURATION_MAX</th>\n",
              "      <th>BLOODPRESSURE_DIASTOLIC_DIFF</th>\n",
              "      <th>BLOODPRESSURE_SISTOLIC_DIFF</th>\n",
              "      <th>HEART_RATE_DIFF</th>\n",
              "      <th>RESPIRATORY_RATE_DIFF</th>\n",
              "      <th>TEMPERATURE_DIFF</th>\n",
              "      <th>OXYGEN_SATURATION_DIFF</th>\n",
              "      <th>BLOODPRESSURE_DIASTOLIC_DIFF_REL</th>\n",
              "      <th>BLOODPRESSURE_SISTOLIC_DIFF_REL</th>\n",
              "      <th>HEART_RATE_DIFF_REL</th>\n",
              "      <th>RESPIRATORY_RATE_DIFF_REL</th>\n",
              "      <th>TEMPERATURE_DIFF_REL</th>\n",
              "      <th>OXYGEN_SATURATION_DIFF_REL</th>\n",
              "      <th>ICU</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1925.000000</td>\n",
              "      <td>1925.000000</td>\n",
              "      <td>1925.000000</td>\n",
              "      <td>1920.000000</td>\n",
              "      <td>1920.000000</td>\n",
              "      <td>1920.000000</td>\n",
              "      <td>1920.000000</td>\n",
              "      <td>1920.000000</td>\n",
              "      <td>1920.000000</td>\n",
              "      <td>1920.000000</td>\n",
              "      <td>1920.000000</td>\n",
              "      <td>1920.000000</td>\n",
              "      <td>821.000000</td>\n",
              "      <td>821.000000</td>\n",
              "      <td>821.000000</td>\n",
              "      <td>821.000000</td>\n",
              "      <td>821.0</td>\n",
              "      <td>821.000000</td>\n",
              "      <td>821.000000</td>\n",
              "      <td>821.000000</td>\n",
              "      <td>821.000000</td>\n",
              "      <td>821.0</td>\n",
              "      <td>821.000000</td>\n",
              "      <td>821.000000</td>\n",
              "      <td>821.000000</td>\n",
              "      <td>821.000000</td>\n",
              "      <td>821.0</td>\n",
              "      <td>821.000000</td>\n",
              "      <td>821.000000</td>\n",
              "      <td>821.000000</td>\n",
              "      <td>821.000000</td>\n",
              "      <td>821.0</td>\n",
              "      <td>821.000000</td>\n",
              "      <td>821.000000</td>\n",
              "      <td>821.000000</td>\n",
              "      <td>821.000000</td>\n",
              "      <td>821.0</td>\n",
              "      <td>821.000000</td>\n",
              "      <td>821.000000</td>\n",
              "      <td>821.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>821.000000</td>\n",
              "      <td>821.000000</td>\n",
              "      <td>821.0</td>\n",
              "      <td>1240.000000</td>\n",
              "      <td>1240.000000</td>\n",
              "      <td>1240.000000</td>\n",
              "      <td>1177.000000</td>\n",
              "      <td>1231.000000</td>\n",
              "      <td>1239.000000</td>\n",
              "      <td>1240.000000</td>\n",
              "      <td>1240.000000</td>\n",
              "      <td>1240.000000</td>\n",
              "      <td>1177.000000</td>\n",
              "      <td>1231.000000</td>\n",
              "      <td>1239.000000</td>\n",
              "      <td>1240.000000</td>\n",
              "      <td>1240.000000</td>\n",
              "      <td>1240.000000</td>\n",
              "      <td>1177.000000</td>\n",
              "      <td>1231.000000</td>\n",
              "      <td>1239.000000</td>\n",
              "      <td>1240.000000</td>\n",
              "      <td>1240.000000</td>\n",
              "      <td>1240.000000</td>\n",
              "      <td>1177.000000</td>\n",
              "      <td>1231.000000</td>\n",
              "      <td>1239.000000</td>\n",
              "      <td>1240.000000</td>\n",
              "      <td>1240.000000</td>\n",
              "      <td>1240.000000</td>\n",
              "      <td>1177.000000</td>\n",
              "      <td>1231.000000</td>\n",
              "      <td>1239.000000</td>\n",
              "      <td>1240.000000</td>\n",
              "      <td>1240.000000</td>\n",
              "      <td>1240.000000</td>\n",
              "      <td>1177.000000</td>\n",
              "      <td>1231.000000</td>\n",
              "      <td>1239.000000</td>\n",
              "      <td>1925.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>192.000000</td>\n",
              "      <td>0.467532</td>\n",
              "      <td>0.368831</td>\n",
              "      <td>0.108333</td>\n",
              "      <td>0.028125</td>\n",
              "      <td>0.097917</td>\n",
              "      <td>0.019792</td>\n",
              "      <td>0.128125</td>\n",
              "      <td>0.046875</td>\n",
              "      <td>0.213021</td>\n",
              "      <td>0.158333</td>\n",
              "      <td>0.809896</td>\n",
              "      <td>0.528527</td>\n",
              "      <td>0.528527</td>\n",
              "      <td>0.528527</td>\n",
              "      <td>0.528527</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-0.963433</td>\n",
              "      <td>-0.963433</td>\n",
              "      <td>-0.963433</td>\n",
              "      <td>-0.963433</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-0.931121</td>\n",
              "      <td>-0.931121</td>\n",
              "      <td>-0.931121</td>\n",
              "      <td>-0.931121</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-0.310924</td>\n",
              "      <td>-0.310924</td>\n",
              "      <td>-0.310924</td>\n",
              "      <td>-0.310924</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-0.311845</td>\n",
              "      <td>-0.311845</td>\n",
              "      <td>-0.311845</td>\n",
              "      <td>-0.311845</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-0.945928</td>\n",
              "      <td>-0.945928</td>\n",
              "      <td>-0.945928</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.954177</td>\n",
              "      <td>-0.954177</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-0.093631</td>\n",
              "      <td>-0.332600</td>\n",
              "      <td>-0.264701</td>\n",
              "      <td>-0.438754</td>\n",
              "      <td>0.066893</td>\n",
              "      <td>0.743077</td>\n",
              "      <td>-0.097790</td>\n",
              "      <td>-0.338468</td>\n",
              "      <td>-0.268632</td>\n",
              "      <td>-0.435121</td>\n",
              "      <td>0.063798</td>\n",
              "      <td>0.748588</td>\n",
              "      <td>-0.040855</td>\n",
              "      <td>-0.207812</td>\n",
              "      <td>-0.264999</td>\n",
              "      <td>-0.483129</td>\n",
              "      <td>0.326823</td>\n",
              "      <td>0.817565</td>\n",
              "      <td>-0.235001</td>\n",
              "      <td>-0.399582</td>\n",
              "      <td>-0.282029</td>\n",
              "      <td>-0.316753</td>\n",
              "      <td>0.014964</td>\n",
              "      <td>0.818593</td>\n",
              "      <td>-0.752454</td>\n",
              "      <td>-0.728053</td>\n",
              "      <td>-0.754100</td>\n",
              "      <td>-0.703683</td>\n",
              "      <td>-0.770338</td>\n",
              "      <td>-0.887196</td>\n",
              "      <td>-0.786997</td>\n",
              "      <td>-0.715950</td>\n",
              "      <td>-0.817800</td>\n",
              "      <td>-0.719147</td>\n",
              "      <td>-0.771327</td>\n",
              "      <td>-0.886982</td>\n",
              "      <td>0.267532</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>111.168431</td>\n",
              "      <td>0.499074</td>\n",
              "      <td>0.482613</td>\n",
              "      <td>0.310882</td>\n",
              "      <td>0.165373</td>\n",
              "      <td>0.297279</td>\n",
              "      <td>0.139320</td>\n",
              "      <td>0.334316</td>\n",
              "      <td>0.211426</td>\n",
              "      <td>0.409549</td>\n",
              "      <td>0.365148</td>\n",
              "      <td>0.392485</td>\n",
              "      <td>0.224100</td>\n",
              "      <td>0.224100</td>\n",
              "      <td>0.224100</td>\n",
              "      <td>0.224100</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.160870</td>\n",
              "      <td>0.160870</td>\n",
              "      <td>0.160870</td>\n",
              "      <td>0.160870</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.169509</td>\n",
              "      <td>0.169509</td>\n",
              "      <td>0.169509</td>\n",
              "      <td>0.169509</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.100256</td>\n",
              "      <td>0.100256</td>\n",
              "      <td>0.100256</td>\n",
              "      <td>0.100256</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.118812</td>\n",
              "      <td>0.118812</td>\n",
              "      <td>0.118812</td>\n",
              "      <td>0.118812</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.076610</td>\n",
              "      <td>0.076610</td>\n",
              "      <td>0.076610</td>\n",
              "      <td>...</td>\n",
              "      <td>0.123582</td>\n",
              "      <td>0.123582</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.252064</td>\n",
              "      <td>0.274102</td>\n",
              "      <td>0.246760</td>\n",
              "      <td>0.217113</td>\n",
              "      <td>0.242858</td>\n",
              "      <td>0.132635</td>\n",
              "      <td>0.257733</td>\n",
              "      <td>0.277952</td>\n",
              "      <td>0.252709</td>\n",
              "      <td>0.225554</td>\n",
              "      <td>0.249208</td>\n",
              "      <td>0.125994</td>\n",
              "      <td>0.281304</td>\n",
              "      <td>0.277802</td>\n",
              "      <td>0.272725</td>\n",
              "      <td>0.278239</td>\n",
              "      <td>0.216198</td>\n",
              "      <td>0.283453</td>\n",
              "      <td>0.271123</td>\n",
              "      <td>0.287580</td>\n",
              "      <td>0.296247</td>\n",
              "      <td>0.402675</td>\n",
              "      <td>0.276163</td>\n",
              "      <td>0.141316</td>\n",
              "      <td>0.364001</td>\n",
              "      <td>0.408677</td>\n",
              "      <td>0.366349</td>\n",
              "      <td>0.482097</td>\n",
              "      <td>0.319001</td>\n",
              "      <td>0.296147</td>\n",
              "      <td>0.324754</td>\n",
              "      <td>0.419103</td>\n",
              "      <td>0.270217</td>\n",
              "      <td>0.446600</td>\n",
              "      <td>0.317694</td>\n",
              "      <td>0.296772</td>\n",
              "      <td>0.442787</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>96.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.605263</td>\n",
              "      <td>0.605263</td>\n",
              "      <td>0.605263</td>\n",
              "      <td>0.605263</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-0.317073</td>\n",
              "      <td>-0.317073</td>\n",
              "      <td>-0.317073</td>\n",
              "      <td>-0.317073</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-0.317073</td>\n",
              "      <td>-0.317073</td>\n",
              "      <td>-0.317073</td>\n",
              "      <td>-0.317073</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-0.966510</td>\n",
              "      <td>-0.966510</td>\n",
              "      <td>-0.966510</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.978877</td>\n",
              "      <td>-0.978877</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-0.262708</td>\n",
              "      <td>-0.523077</td>\n",
              "      <td>-0.420791</td>\n",
              "      <td>-0.552542</td>\n",
              "      <td>-0.102991</td>\n",
              "      <td>0.684211</td>\n",
              "      <td>-0.283951</td>\n",
              "      <td>-0.538462</td>\n",
              "      <td>-0.433962</td>\n",
              "      <td>-0.517241</td>\n",
              "      <td>-0.107143</td>\n",
              "      <td>0.684211</td>\n",
              "      <td>-0.195876</td>\n",
              "      <td>-0.375000</td>\n",
              "      <td>-0.452991</td>\n",
              "      <td>-0.642857</td>\n",
              "      <td>0.186813</td>\n",
              "      <td>0.818182</td>\n",
              "      <td>-0.418803</td>\n",
              "      <td>-0.578378</td>\n",
              "      <td>-0.477612</td>\n",
              "      <td>-0.575758</td>\n",
              "      <td>-0.188406</td>\n",
              "      <td>0.736842</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>192.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.605263</td>\n",
              "      <td>0.605263</td>\n",
              "      <td>0.605263</td>\n",
              "      <td>0.605263</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-0.317073</td>\n",
              "      <td>-0.317073</td>\n",
              "      <td>-0.317073</td>\n",
              "      <td>-0.317073</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-0.317073</td>\n",
              "      <td>-0.317073</td>\n",
              "      <td>-0.317073</td>\n",
              "      <td>-0.317073</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-0.938950</td>\n",
              "      <td>-0.938950</td>\n",
              "      <td>-0.938950</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.978029</td>\n",
              "      <td>-0.978029</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-0.100172</td>\n",
              "      <td>-0.374405</td>\n",
              "      <td>-0.283019</td>\n",
              "      <td>-0.502825</td>\n",
              "      <td>0.035714</td>\n",
              "      <td>0.736842</td>\n",
              "      <td>-0.135802</td>\n",
              "      <td>-0.384615</td>\n",
              "      <td>-0.283019</td>\n",
              "      <td>-0.517241</td>\n",
              "      <td>0.035714</td>\n",
              "      <td>0.736842</td>\n",
              "      <td>-0.030928</td>\n",
              "      <td>-0.250000</td>\n",
              "      <td>-0.282051</td>\n",
              "      <td>-0.500000</td>\n",
              "      <td>0.318681</td>\n",
              "      <td>0.878788</td>\n",
              "      <td>-0.247863</td>\n",
              "      <td>-0.459459</td>\n",
              "      <td>-0.328358</td>\n",
              "      <td>-0.454545</td>\n",
              "      <td>-0.014493</td>\n",
              "      <td>0.842105</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-0.987730</td>\n",
              "      <td>-0.984733</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-0.976190</td>\n",
              "      <td>-0.979798</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-0.984944</td>\n",
              "      <td>-0.989822</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-0.975924</td>\n",
              "      <td>-0.980333</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>288.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.605263</td>\n",
              "      <td>0.605263</td>\n",
              "      <td>0.605263</td>\n",
              "      <td>0.605263</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-0.958115</td>\n",
              "      <td>-0.958115</td>\n",
              "      <td>-0.958115</td>\n",
              "      <td>-0.958115</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-0.317073</td>\n",
              "      <td>-0.317073</td>\n",
              "      <td>-0.317073</td>\n",
              "      <td>-0.317073</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-0.317073</td>\n",
              "      <td>-0.317073</td>\n",
              "      <td>-0.317073</td>\n",
              "      <td>-0.317073</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-0.938950</td>\n",
              "      <td>-0.938950</td>\n",
              "      <td>-0.938950</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.968315</td>\n",
              "      <td>-0.968315</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>0.086420</td>\n",
              "      <td>-0.184615</td>\n",
              "      <td>-0.132075</td>\n",
              "      <td>-0.383289</td>\n",
              "      <td>0.205890</td>\n",
              "      <td>0.823995</td>\n",
              "      <td>0.086420</td>\n",
              "      <td>-0.200000</td>\n",
              "      <td>-0.132075</td>\n",
              "      <td>-0.379310</td>\n",
              "      <td>0.196429</td>\n",
              "      <td>0.842105</td>\n",
              "      <td>0.175258</td>\n",
              "      <td>-0.050000</td>\n",
              "      <td>-0.094017</td>\n",
              "      <td>-0.357143</td>\n",
              "      <td>0.472527</td>\n",
              "      <td>0.919192</td>\n",
              "      <td>-0.076923</td>\n",
              "      <td>-0.243243</td>\n",
              "      <td>-0.119403</td>\n",
              "      <td>-0.212121</td>\n",
              "      <td>0.217391</td>\n",
              "      <td>0.894737</td>\n",
              "      <td>-0.565217</td>\n",
              "      <td>-0.558282</td>\n",
              "      <td>-0.541985</td>\n",
              "      <td>-0.647059</td>\n",
              "      <td>-0.595238</td>\n",
              "      <td>-0.878788</td>\n",
              "      <td>-0.645482</td>\n",
              "      <td>-0.522176</td>\n",
              "      <td>-0.662529</td>\n",
              "      <td>-0.634409</td>\n",
              "      <td>-0.594677</td>\n",
              "      <td>-0.880155</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>384.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 229 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       PATIENT_VISIT_IDENTIFIER  ...          ICU\n",
              "count               1925.000000  ...  1925.000000\n",
              "mean                 192.000000  ...     0.267532\n",
              "std                  111.168431  ...     0.442787\n",
              "min                    0.000000  ...     0.000000\n",
              "25%                   96.000000  ...     0.000000\n",
              "50%                  192.000000  ...     0.000000\n",
              "75%                  288.000000  ...     1.000000\n",
              "max                  384.000000  ...     1.000000\n",
              "\n",
              "[8 rows x 229 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6yOF1npKuNVj",
        "outputId": "93df3815-a38b-4fd5-c3a2-b6851be2563b"
      },
      "source": [
        "dados['ICU'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    1410\n",
              "1     515\n",
              "Name: ICU, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8aZ92HfxWxd"
      },
      "source": [
        "#### **Desafio - Pesquisar Aprendizado Supervisionado, Não Supervisionado, Por Reforço e Mais**\n",
        "---\n",
        "\n",
        "**Aprendizado Supervisionado**\n",
        "\n",
        "É o modelo mais comum. Treinamos o algoritmo com um volume inicial de dados, mostrando características e o resultado obtido. O processo de aprendizado leva a prever a saída a partir de novos dados de entrada.\n",
        "\n",
        "**Aprendizado Não Supervisionada**\n",
        "\n",
        "É o modelo utilizado quando não temos os dados de resultado obtido na base de dados inicial. O algoritmo tenta entender as características e gerar agrupamentos (*clusters*), muitas vezes para depois passar cada *cluster* por um processo de aprendizado supervisionado.\n",
        "\n",
        "**Aprendizado Por Reforço**\n",
        "\n",
        "É o modelo mais indicado para tomada de decisões. Funciona por tentativa e erro, onde a máquina é exposta a vários processos decisórios e vai aprendendo conforme as decisões tomadas forem acertadas ou não.\n",
        "\n",
        "**Redes Neurais e** ***Deep Learning***\n",
        "\n",
        "Trabalha com sucessivas camadas de aprendizado para melhorar o processo de tomada de decisão. É utilizado para descobrir padrões em dados desestruturados, ideal para reconhecimento de imagens e de fala, por exemplo.\n",
        "\n",
        "Normalmente as camadas são formadas por combinações de algoritmos de aprendizado supervisionado e não supervisionado.\n",
        "\n",
        "Fontes:\n",
        "\n",
        "[Data Hackers](https://medium.com/data-hackers/simplificando-as-categorias-de-algoritmos-de-machine-learning-191ac9dc8ddf)\n",
        "\n",
        "[IBM - Machine Learning for Dummies](https://www.ibm.com/downloads/cas/GB8ZMQZ3)\n",
        "\n",
        "[LAMFO - UNB](https://lamfo-unb.github.io/2017/07/27/tres-tipos-am/)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjG2jxXTxUES"
      },
      "source": [
        "#### **Desafio - Pesquisar Sobre Regressão, Classificação, etc.**\n",
        "---\n",
        "\n",
        "A diferença entre a **Regressão** e a **Classificação** se dá normalmente pelo tipo de resultado que teremos.\n",
        "\n",
        "Para modelos de **Classificação** teremos um número limitado e discreto de resultados.\n",
        "\n",
        "Já para modelos de **Regressão** o resultado esperado está numa faixa contínua de dados. É mais utilizado para encontrar a correlação entre variáveis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3wkvRFRtodS"
      },
      "source": [
        "### **O que aprendemos nessa aula?**\n",
        "---\n",
        "\n",
        ">* Como usar os notebooks direto do **Kaggle**;\n",
        ">\n",
        ">* O que são algoritmos supervisionados;\n",
        ">\n",
        ">* O que são dados anonimizados e sua relevância da perspectiva ética.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxseMv1OMe13"
      },
      "source": [
        "## **Aula 03 - Modelos de** ***Machine Learning***\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2FEf0q76bwf"
      },
      "source": [
        "Para iniciar o nosso estudo, vamos usar a biblioteca **```SciKitLearn```** e o seu modelo de Regressão Logística."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9M2KuHNLNIXM",
        "outputId": "e083c2cb-31e9-483f-da05-fd5381bb0b1a"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "modelo = LogisticRegression()\n",
        "\n",
        "modelo"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHNdacNN6pL_"
      },
      "source": [
        "Vamos preparar a nossa base de dados para manter só as colunas com números, deixar de fora as linhas com dados nulos e NaN e também a coluna 'ICU', que é a coluna que contém o resultado.\n",
        "\n",
        "Essa coluna 'ICU' vai para uma tabela em separado, para avaliarmos o resultado depois."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yevmA2UmNNb5"
      },
      "source": [
        "# Pegar só as colunas que contém números\n",
        "x_columns = dados.describe().columns\n",
        "\n",
        "# Excluir linhas que contém NaN\n",
        "dados_limpos = dados.dropna()\n",
        "\n",
        "# Separar os dados de resultado (y) e de entrada (x)\n",
        "y = dados_limpos.ICU\n",
        "x = dados_limpos[x_columns].drop(['ICU'], axis = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IcIM1iUoNmXk",
        "outputId": "e3efee61-e227-4d13-d35d-012bb2b43c5d"
      },
      "source": [
        "modelo.fit(x,y)\n",
        "prev = modelo.predict(x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUClyxUwPLLP",
        "outputId": "f0614d21-e850-4d4b-8e90-3575f2170d52"
      },
      "source": [
        "print(sum(prev == y),'acertos')\n",
        "print(sum(prev != y),'erros')\n",
        "print(sum(prev == y) / len(x) * 100,'% de acertos')\n",
        "print(sum(prev != y) / len(x) * 100,'% de erros')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "496 acertos\n",
            "49 erros\n",
            "91.00917431192661 % de acertos\n",
            "8.990825688073395 % de erros\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ld104y0WP556"
      },
      "source": [
        "#### **Desafio - Pesquisar Sobre Ética em** ***Data Science*** **e Inteligência Artificial**\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiEbORgFhInm"
      },
      "source": [
        "https://insider.dn.pt/em-rede/privacidade-no-data-science-a-etica-deve-ir-muito-alem-da-lei/25736/\n",
        "\n",
        "https://www.serpro.gov.br/lgpd/noticias/2019/tratamento-etico-dados-pessoais-ciencia-data-science\n",
        "\n",
        "http://www.sciencefriday.com/articles/10-questions-for-the-nations-first-chief-data-scientist/\n",
        "\n",
        "https://medium.com/somos-tera/o-que-e-etica-de-dados-3ce4702cdfc1\n",
        "\n",
        "https://royalsocietypublishing.org/doi/full/10.1098/rsta.2016.0360#sec-1\n",
        "\n",
        "https://www.itinsight.pt/news/data-and-analytics/insights-2018---etica-estrategia-e-futuro-da-data-science-em-debate\n",
        "\n",
        "https://neofeed.com.br/blog/home/qual-e-a-etica-da-inteligencia-artificial/\n",
        "\n",
        "https://pt.wikipedia.org/wiki/%C3%89tica_na_intelig%C3%AAncia_artificial\n",
        "\n",
        "https://www.umov.me/inteligencia-artificial-etica-e-trabalho/\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnffWCuDRq__"
      },
      "source": [
        "#### **Desafio - Qual o motivo de jogar fora o ICU dos dados X? O que acontece se estiver nos dados X? Isso seria bom ou ruim?**\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJgPMvT6eh2A"
      },
      "source": [
        "Como podemos ver no resultado do código abaixo, se rodarmos o modelo com os dados da coluna 'ICU' inclusas no parâmetro X, o modelo será treinado para responder com 100% de acerto para esses dados de treino, pois ele acaba aprendendo que sempre que receber no *input* que 'ICU' = 1, ele vai responder que o *output*, que também é 'ICU' será 1, o que é óbvio.\n",
        "\n",
        "Mas esse modelo não vai ter nenhuma utilidade, pois vai depender de termos a resposta da coluna 'ICU' para nos dar a previsão de resposta (que já teremos). Passa a ser um modelo de previsão do passado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CsDcyCatT1ot",
        "outputId": "56234d52-c15b-4d69-eb4c-143156d59b97"
      },
      "source": [
        "modelo2 = LogisticRegression();\n",
        "x2 = dados_limpos[x_columns]\n",
        "modelo2.fit(x2,y);\n",
        "print(sum(modelo2.predict(x2) == y),'acertos');\n",
        "print(sum(modelo2.predict(x2) != y),'erros');\n",
        "print(sum(modelo2.predict(x2) == y) / len(x2) * 100,'% de acertos');\n",
        "print(sum(modelo2.predict(x2) != y) / len(x2) * 100,'% de erros');"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "545 acertos\n",
            "0 erros\n",
            "100.0 % de acertos\n",
            "0.0 % de erros\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIJSEOjv5ZW2"
      },
      "source": [
        "### **O que aprendemos nessa aula?**\n",
        "---\n",
        "\n",
        ">* O que é a uma regressão logística;\n",
        ">\n",
        ">* Como treinar o seu primeiro modelo de ***Machine Learning***.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNO1rPAp6P6O"
      },
      "source": [
        "## **Aula 04 - Métricas e Avaliações**\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70XgTCAK9UmY"
      },
      "source": [
        "Vamos melhorar o nosso modelo, mas antes vamos ver qual seria a classificação de um modelo **```DummyClassifier```**, que simplesmente classifica tudo com base na proporção que temos na base de dados.\n",
        "\n",
        "Vamos verificar qual a proporção da base de dados."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzsEX20O9jmm",
        "outputId": "a774f664-7be6-4d65-a898-8bb1aa6f752d"
      },
      "source": [
        "y.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    281\n",
              "1    264\n",
              "Name: ICU, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSAi-XB99oFI"
      },
      "source": [
        "Vemos que o resultado 0 está em 281 linhas, ou seja, 51,56% da base."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DusjU_6H9zim",
        "outputId": "41087808-2018-4af5-ffb4-b1a8ba76d72b"
      },
      "source": [
        "from sklearn.dummy import DummyClassifier\n",
        "\n",
        "modelo_dummy = DummyClassifier()\n",
        "modelo_dummy.fit(x,y)\n",
        "prev_dummy = modelo_dummy.predict(x)\n",
        "print(sum(prev_dummy == y) / len(x) * 100,'% de acertos');"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "51.559633027522935 % de acertos\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/dummy.py:132: FutureWarning: The default value of strategy will change from stratified to prior in 0.24.\n",
            "  \"stratified to prior in 0.24.\", FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qptejmnn-mG-",
        "outputId": "77282afb-7ecd-46b5-eeb4-00a6d62e0c84"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "teste = np.bincount(prev_dummy) ## Equivalente ao value_counts para arrays\n",
        "ii = np.nonzero(teste)[0]\n",
        "\n",
        "np.vstack((ii,teste[ii])).T"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0, 289],\n",
              "       [  1, 256]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMY7yRwbBdqd"
      },
      "source": [
        "A biblioteca **```sklearn```** também tem uma ferramenta de avaliação de acurácia do modelo, para nos permitir comparações.\n",
        "\n",
        "No presente caso, a resposta vai ser a mesma que já calculamos acima."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mU-bt2v1Aso4",
        "outputId": "481cf690-ea62-4765-8c4b-10e1c9ed78d3"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "accuracy_score(y, prev_dummy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5155963302752293"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G80zXSfbB2fG",
        "outputId": "720aabf0-33e3-4397-bba1-90667fd64c50"
      },
      "source": [
        "accuracy_score(y, modelo2.predict(x2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4voVYBWB1PN",
        "outputId": "ed90508c-fc5e-45e3-8c92-71388bc472d2"
      },
      "source": [
        "accuracy_score(y, prev)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9100917431192661"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Brn2beEHCzTX"
      },
      "source": [
        "O problema de fazermos o treino com toda a base é que isso aumenta artificialmente a acurácia do modelo, pois ele vai ser testado contra dados que também foram usados no treino.\n",
        "\n",
        "Para que o modelo esteja preparado para trabalhar com dados novos, precisamos testar a acurácia contra dados que não foram usados no treino.\n",
        "\n",
        "Para isso, vamos agora fazer a separação dos nossos dados em treino e teste, para poder criar um modelo e depois verificar a acurácia contra dados que ele nunca viu antes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SY_jVpNmDDyI"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOEUwirCDfWu",
        "outputId": "845882d2-43da-40bb-acae-99815e418770"
      },
      "source": [
        "print(len(x_train), len(y_train))\n",
        "print(len(x_test), len(y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "408 408\n",
            "137 137\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8jvODxXDshg",
        "outputId": "0e57d78b-04ea-4190-c6ab-861fff6d028f"
      },
      "source": [
        "modelo_2 = LogisticRegression()\n",
        "modelo_2.fit(x_train, y_train)\n",
        "\n",
        "y_prev = modelo_2.predict(x_test)\n",
        "accuracy_score(y_test, y_prev)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9051094890510949"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ibOeL7UBF6w-",
        "outputId": "e30cb720-481f-4c2a-9248-ed7fd5c1eaf7"
      },
      "source": [
        "np.random.seed(73246)\n",
        "\n",
        "x3_train, x3_test, y3_train, y3_test = train_test_split(x, y, stratify = y)\n",
        "print(len(x3_train), len(y3_train))\n",
        "print(len(x3_test), len(y3_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "408 408\n",
            "137 137\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wTm9uhmpGUVI",
        "outputId": "d07cd4c8-cb68-4965-fe23-85eb0d654a62"
      },
      "source": [
        "modelo_3 = LogisticRegression()\n",
        "modelo_3.fit(x3_train, y3_train)\n",
        "\n",
        "y3_prev = modelo_3.predict(x3_test)\n",
        "accuracy_score(y3_test, y3_prev)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8321167883211679"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYP3-UzSHd8l"
      },
      "source": [
        "#### **Desafio - Decida quanto você vai escolher para treino e quanto para teste? Quanto as pessoas utilizam e qual o motivo?**\n",
        "---\n",
        "\n",
        "A divisão mais comum é na proporção 70%\\~30% entre dados de treino e de teste. Gosto de usar essa proporção sempre que possível. No caso específico do COVID-19 tenho encontrado dificuldade para utilizar essa proporção em razão da pequena quantidade de dados disponíveis.\n",
        "\n",
        "No módulo anterior foi utilizado quase 84% (400 das 478 linhas) dos dados para treino, justamente por causa disso.\n",
        "\n",
        "A função **```train_test_split```** faz a divisão tendo como parâmetro *default* a proporção de 75%\\~25%, mas isso pode ser alterado mexendo no parâmetro **```test_size```**, que recebe um número entre 0 e 1 que vai indicar a proporção do teste (o resto é treino).\n",
        "\n",
        "Para fazer uma divisão entre dados de treino, de teste e de validação, encontrei a função abaixo.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CrBVtg0YonCY"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def split_stratified_into_train_val_test(df_input, stratify_colname='y',\n",
        "                                         frac_train=0.6, frac_val=0.15, frac_test=0.25,\n",
        "                                         random_state=None):\n",
        "    '''\n",
        "    Splits a Pandas dataframe into three subsets (train, val, and test)\n",
        "    following fractional ratios provided by the user, where each subset is\n",
        "    stratified by the values in a specific column (that is, each subset has\n",
        "    the same relative frequency of the values in the column). It performs this\n",
        "    splitting by running train_test_split() twice.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_input : Pandas dataframe\n",
        "        Input dataframe to be split.\n",
        "    stratify_colname : str\n",
        "        The name of the column that will be used for stratification. Usually\n",
        "        this column would be for the label.\n",
        "    frac_train : float\n",
        "    frac_val   : float\n",
        "    frac_test  : float\n",
        "        The ratios with which the dataframe will be split into train, val, and\n",
        "        test data. The values should be expressed as float fractions and should\n",
        "        sum to 1.0.\n",
        "    random_state : int, None, or RandomStateInstance\n",
        "        Value to be passed to train_test_split().\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    df_train, df_val, df_test :\n",
        "        Dataframes containing the three splits.\n",
        "    '''\n",
        "\n",
        "    if frac_train + frac_val + frac_test != 1.0:\n",
        "        raise ValueError('fractions %f, %f, %f do not add up to 1.0' % \\\n",
        "                         (frac_train, frac_val, frac_test))\n",
        "\n",
        "    if stratify_colname not in df_input.columns:\n",
        "        raise ValueError('%s is not a column in the dataframe' % (stratify_colname))\n",
        "\n",
        "    X = df_input # Contains all columns.\n",
        "    y = df_input[[stratify_colname]] # Dataframe of just the column on which to stratify.\n",
        "\n",
        "    # Split original dataframe into train and temp dataframes.\n",
        "    df_train, df_temp, y_train, y_temp = train_test_split(X,\n",
        "                                                          y,\n",
        "                                                          stratify=y,\n",
        "                                                          test_size=(1.0 - frac_train),\n",
        "                                                          random_state=random_state)\n",
        "\n",
        "    # Split the temp dataframe into val and test dataframes.\n",
        "    relative_frac_test = frac_test / (frac_val + frac_test)\n",
        "    df_val, df_test, y_val, y_test = train_test_split(df_temp,\n",
        "                                                      y_temp,\n",
        "                                                      stratify=y_temp,\n",
        "                                                      test_size=relative_frac_test,\n",
        "                                                      random_state=random_state)\n",
        "\n",
        "    assert len(df_input) == len(df_train) + len(df_val) + len(df_test)\n",
        "\n",
        "    return df_train, df_val, df_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGn1mjgIHdxU"
      },
      "source": [
        "#### **Desafio - Olhe a documentação da ```LogisticRegression```... o que tem de parâmetro em comum com o ```train_test_split``` e o que isso significa para nós?**\n",
        "---\n",
        "\n",
        "O parâmetro que ambos têm em comum é o **```random_state```**.\n",
        "\n",
        "Acredito que sirva para garantir a reprodutibilidade do estudo, mas no caso da Regressão Logística, esse parâmetro só tem efeito para alguns algoritmos (e o *default* não está entre eles)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqrBm_j77AGN"
      },
      "source": [
        "#### **OBS.:** Não consegui atingir o mesmo resultado que o vídeo da aula.\n",
        "\n",
        "Copiei o código da aula disponibilizado abaixo, idêntico, em uma célula única, abaixo. Mesmo assim, não fecha o resultado, não entendi..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yhyxu8lGmB2v",
        "outputId": "93485089-8c9b-4ea8-eecf-c55820439c2d"
      },
      "source": [
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "dados = pd.read_excel(\"https://github.com/gcochlar/Bootcamp_DataScience/blob/main/dados/Kaggle_Sirio_Libanes_ICU_Prediction.xlsx?raw=true\")\n",
        "dados.head()\n",
        "\n",
        "dados.describe()\n",
        "\n",
        "dados[\"ICU\"]\n",
        "\n",
        "dados[\"ICU\"].value_counts()\n",
        "\n",
        "x_columns = dados.describe().columns\n",
        "\n",
        "dados_limpos = dados.dropna()\n",
        "\n",
        "y = dados_limpos[\"ICU\"]\n",
        "x = dados_limpos[x_columns].drop([\"ICU\"], axis=1)\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "modelo = LogisticRegression()\n",
        "modelo.fit(x, y)\n",
        "\n",
        "modelo.predict([x.iloc[4]])\n",
        "\n",
        "x.iloc[4]\n",
        "\n",
        "y.iloc[4]\n",
        "\n",
        "sum(modelo.predict(x) == y)\n",
        "\n",
        "sum(modelo.predict(x) != y)\n",
        "\n",
        "accuracy = sum(modelo.predict(x) == y) / len(y) * 100\n",
        "accuracy\n",
        "\n",
        "y.value_counts()\n",
        "\n",
        "from sklearn.dummy import DummyClassifier\n",
        "\n",
        "modelo = DummyClassifier(\"most_frequent\")\n",
        "modelo.fit(x, y)\n",
        "sum(modelo.predict(x) == y) / len(y) * 100\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "accuracy_score(y, modelo.predict(x))\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "np.random.seed(73246)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, stratify=y)\n",
        "\n",
        "print(len(x_train), len(y_train))\n",
        "print(len(x_test), len(y_test))\n",
        "\n",
        "modelo = LogisticRegression()\n",
        "modelo.fit(x_train, y_train)\n",
        "\n",
        "y_prediction = modelo.predict(x_test)\n",
        "accuracy_score(y_test, y_prediction)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "408 408\n",
            "137 137\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8321167883211679"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVv2Omx3mN3o"
      },
      "source": [
        "No *notebook* que tem só o conteúdo da aula, o resultado é 0.8248175182481752.\n",
        "\n",
        "A mesma célula, copiada para cá, dá esse resultado acima (0.8321167883211679). Isso não tem explicação..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBHSMdl8RncQ"
      },
      "source": [
        "### **O que aprendemos nessa aula?**\n",
        "---\n",
        "\n",
        ">* Como metrificar um modelos de classificação, usando o **```Sci-kit Learn```**;\n",
        ">\n",
        ">* Como realizar o treinamento dividindo em dados de treino e teste.\n",
        "\n",
        "---\n"
      ]
    }
  ]
}